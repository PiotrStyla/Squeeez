# üî¨ PHASE 4 DAY 1: STARLIT LSTM Analysis & Implementation Plan

**Date:** December 1, 2025  
**Status:** Deep Dive Complete - Ready to Implement!  
**Goal:** Understand STARLIT's LSTM and adapt for PAQ8px

---

## üìö STARLIT LSTM ARCHITECTURE - COMPLETE BREAKDOWN

### **Key Files Analyzed:**
```
‚úÖ lstm.h (35 lines) - Main LSTM class interface
‚úÖ lstm.cpp (147 lines) - Training and prediction logic
‚úÖ lstm-layer.h (60 lines) - LSTM layer with gates
‚úÖ lstm-layer.cpp (206 lines) - Forward/backward pass, Adam optimizer
‚úÖ mixer.h (37 lines) - Context-based mixer
```

---

## üß† HOW STARLIT'S LSTM WORKS

### **1. Main LSTM Class (`Lstm`)**

```cpp
Lstm::Lstm(
    unsigned int input_size,      // Number of model predictions (e.g., 9-13 from PAQ8px)
    unsigned int output_size,     // Output classes (256 for byte prediction)
    unsigned int num_cells,       // LSTM hidden units (32-64 typical)
    unsigned int num_layers,      // Number of LSTM layers (1-2)
    int horizon,                  // Lookback window (training history)
    float learning_rate,          // Training rate
    float gradient_clip           // Prevent gradient explosion
)
```

**Key Methods:**
- `SetInput()` - Set current model predictions
- `Predict()` - Forward pass, get prediction
- `Perceive()` - Backward pass, update weights (training)
- `SaveToDisk()` / `LoadFromDisk()` - Save/load trained weights

### **2. LSTM Layer Structure (`LstmLayer`)**

**Classic LSTM Gates:**
```cpp
// Forget gate: What to forget from cell state
forget_gate_ = sigmoid(W_f * input + b_f)

// Input gate: What new info to add
input_node_ = tanh(W_i * input + b_i)
input_gate_ = 1.0 - forget_gate_  // Complementary

// Cell state update
state = state * forget_gate + input_node * input_gate

// Output gate: What to output
output_gate_ = sigmoid(W_o * input + b_o)
hidden = output_gate * tanh(state)
```

**Training: Adam Optimizer**
```cpp
// Adaptive learning rate
// Beta1 = 0.025, Beta2 = 0.9999
// First-order momentum (m) + second-order (v)
m = beta1 * m + (1 - beta1) * gradient
v = beta2 * v + (1 - beta2) * gradient¬≤
weights -= learning_rate * m / sqrt(v + eps)
```

### **3. Integration Flow**

```
INPUT: Model predictions (9-13 values from PAQ8px models)
  ‚Üì
LSTM Layer(s): Process with gates (forget, input, output)
  ‚Üì
Hidden State: Combined representation
  ‚Üì
Output Layer: Linear projection to 256 classes (byte values)
  ‚Üì
Softmax: Probability distribution
  ‚Üì
PREDICTION: Next byte probability
```

---

## üí° KEY INSIGHTS FROM CODE ANALYSIS

### **1. Training Strategy:**

STARLIT uses **online learning**:
- Predicts next byte
- Observes actual byte
- Backpropagates error immediately
- Updates weights continuously

**This is PERFECT for compression** because:
- Learns patterns specific to current data
- Adapts to local context
- No separate training phase needed!

### **2. Input Format:**

```cpp
// Inputs to LSTM:
- Model predictions (from PAQ8px models)
- Previous hidden state (recurrent connection)
- Context features (current position, recent bytes)
- Bias term (constant 1.0)

Total input size: ~20-30 features
```

### **3. Horizon (Lookback Window):**

```cpp
int horizon = 200;  // Typical value

// Stores last 200 steps for:
- Backpropagation through time (BPTT)
- Context awareness
- Gradient computation
```

### **4. Weight Initialization:**

```cpp
// Xavier/Glorot initialization
float val = sqrt(6.0 / (input_size + output_size));
weights = uniform(-val, +val)

// Bias for forget gate = 1.0 (initially remember everything)
```

### **5. Gradient Clipping:**

```cpp
// Prevents exploding gradients
if (gradient > gradient_clip) gradient = gradient_clip;
if (gradient < -gradient_clip) gradient = -gradient_clip;

// Typical clip value: 1.0 to 10.0
```

---

## üéØ ADAPTATION PLAN FOR PAQ8PX

### **Architecture Design:**

```cpp
// PAQ8px Integration
Input:
  - 9-13 model predictions from PAQ8px
  - Recent bytes (context)
  - Current mixing weights (optional)
  - Total: ~15-20 inputs

LSTM:
  - 1-2 layers
  - 32-64 cells per layer
  - Online learning during compression

Output:
  - Single prediction (bit 0 or 1)
  - OR: Byte probability (8 bits)
```

### **Two Integration Approaches:**

#### **Approach A: Replace Mixer (Aggressive)**
```cpp
// Replace PAQ8px's mixer entirely with LSTM
// Pro: Maximum LSTM benefit
// Con: Risky, loses PAQ8px's tuned mixer
// Recommendation: Try second!
```

#### **Approach B: Hybrid Mixer (Safe)** ‚úÖ **RECOMMENDED**
```cpp
// Keep PAQ8px mixer, add LSTM as additional input
// Pro: Best of both worlds, safer
// Con: Slightly more complex
// Recommendation: Start here!

final_prediction = 
    weight_paq8px * paq8px_mixer(models) +
    weight_lstm * lstm_mixer(models)
    
// Or: LSTM learns to mix paq8px output with model outputs
```

---

## üõ†Ô∏è IMPLEMENTATION ROADMAP

### **Phase A: Standalone LSTM (Days 1-2)**

**Step 1: Create Basic LSTM Class**
```cpp
// File: C:\HutterLab\paq8px\model\LstmMixer.hpp
class LstmMixer {
    public:
        LstmMixer(int num_inputs, int num_cells, int num_layers);
        float predict(const std::vector<float>& inputs);
        void learn(int actual_bit);
    private:
        // LSTM layers
        // Weights
        // Hidden state
};
```

**Step 2: Implement LSTM Gates**
```cpp
// Forget gate, input gate, output gate
// Forward pass
// Backward pass (simplified)
```

**Step 3: Test Standalone**
```cpp
// Create simple test
// Feed random inputs
// Verify output
// Check training updates weights
```

### **Phase B: PAQ8px Integration (Days 3-4)**

**Step 4: Add Data Collection**
```cpp
// In PAQ8px mixer:
// Collect model predictions
// Store in buffer for LSTM
```

**Step 5: Integrate LSTM**
```cpp
// Option 1: Parallel mixing
mixer_prediction = paq8px_mix(models);
lstm_prediction = lstm_mix(models);
final = blend(mixer_prediction, lstm_prediction);

// Option 2: Sequential
features = extract_from_paq8px();
lstm_prediction = lstm_mix(features);
final = lstm_prediction;
```

**Step 6: Add Training Loop**
```cpp
// After each bit prediction:
actual_bit = compressed_bit;
lstm.learn(actual_bit);  // Update weights
```

### **Phase C: Testing (Day 5)**

**Step 7: Test on 10 MB**
```cpp
// Compile modified PAQ8px
// Run on enwik_10mb_reordered_transformed
// Compare:
//   - Baseline (without LSTM)
//   - With LSTM
// Target: >2.5% improvement
```

**Step 8: Analyze Results**
```cpp
// If good (>2.5%): Scale to enwik9!
// If okay (1.5-2.5%): Tune hyperparameters
// If poor (<1.5%): Debug or pivot
```

### **Phase D: Full enwik9 (Days 6-9)**

**Step 9: Launch Full Test**
```cpp
// Configure for enwik9
// Start 73-hour compression
// Monitor progress
```

**Step 10: RESULTS!**
```cpp
// Analyze compressed size
// Compare to 127.44 MB baseline
// Target: <121 MB (6+ MB improvement)
// Dream: <114 MB (WORLD RECORD!)
```

---

## üìä EXPECTED PARAMETERS

### **LSTM Configuration:**

```cpp
// Input size
int input_size = 15;  // 9-13 PAQ8px models + context

// Architecture
int num_cells = 48;   // Hidden units (32-64 range)
int num_layers = 1;   // Start simple
int horizon = 200;    // Lookback window

// Training
float learning_rate = 0.001;  // Start conservative
float gradient_clip = 5.0;    // Prevent explosion

// Output
int output_size = 2;  // Bit 0 or 1
// OR: output_size = 256 for full byte prediction
```

### **Why These Values?**

- **48 cells:** Balance between capacity and speed
- **1 layer:** Simpler, faster, easier to debug
- **200 horizon:** Enough context, not too memory-heavy
- **0.001 learning rate:** Conservative, stable training
- **5.0 clip:** Prevents gradient explosion without being too restrictive

---

## ‚ö° SIMPLIFICATIONS FOR SPEED

### **To Get Working Fast:**

1. **Skip Full Backprop Through Time**
   - Use truncated BPTT (last 10-20 steps)
   - Or: Simple gradient descent (no Adam initially)
   - Add Adam later if needed

2. **Single Layer First**
   - 1 LSTM layer is easier to debug
   - Add second layer only if helps

3. **Smaller Horizon**
   - Start with horizon=50
   - Increase if helpful

4. **Bit-Level Prediction**
   - Predict bit 0/1 (simpler than full byte)
   - Binary classification

5. **Use Existing Sigmoid/Tanh**
   - PAQ8px has these functions
   - Reuse them!

---

## üîß CONCRETE FIRST IMPLEMENTATION

### **Minimal Working LSTM (Pseudo-Code):**

```cpp
class SimpleLstm {
    private:
        int input_size, num_cells;
        vector<float> forget_weights, input_weights, output_weights;
        vector<float> cell_state, hidden_state;
        float learning_rate;
    
    public:
        SimpleLstm(int in_size, int cells) {
            input_size = in_size;
            num_cells = cells;
            // Initialize weights randomly
            init_weights();
            // Zero states
            cell_state.resize(num_cells, 0);
            hidden_state.resize(num_cells, 0);
            learning_rate = 0.001;
        }
        
        float predict(vector<float>& inputs) {
            // Forget gate
            auto forget = sigmoid(dot(inputs, forget_weights));
            
            // Input gate
            auto input_node = tanh(dot(inputs, input_weights));
            auto input_gate = 1.0 - forget;  // Complementary
            
            // Update cell state
            cell_state = cell_state * forget + input_node * input_gate;
            
            // Output gate
            auto output_gate = sigmoid(dot(inputs, output_weights));
            hidden_state = output_gate * tanh(cell_state);
            
            // Final prediction (simple linear)
            return sigmoid(sum(hidden_state));
        }
        
        void learn(float target) {
            float error = hidden_state[0] - target;  // Simplified
            // Update weights (simplified gradient descent)
            // In reality: backprop through gates
            for (int i = 0; i < weights.size(); i++) {
                weights[i] -= learning_rate * error * inputs[i];
            }
        }
};
```

**This is 80% of what we need!** We can refine later!

---

## üéØ SUCCESS METRICS - 10 MB TEST

### **Go/No-Go Decision:**

```
Baseline (Phase 2):     1,873,130 bytes
Target with LSTM:       1,826,000 bytes (2.5% improvement)

‚úÖ GO (>2.5%):         Scale to enwik9 immediately
‚ö†Ô∏è TUNE (1.5-2.5%):    Optimize hyperparameters first
‚ùå PIVOT (<1.5%):      Try different approach
```

### **What to Measure:**

```cpp
// Compression metrics
- File size (bytes)
- Compression ratio
- Bits per byte
- Improvement vs baseline

// LSTM metrics  
- Training loss over time
- Prediction accuracy
- Weight norms (detect explosions)
- Time per byte (performance)

// Memory usage
- LSTM memory overhead
- Total RAM used
```

---

## üí° DEBUGGING CHECKLIST

### **If LSTM Doesn't Help:**

1. **Check weights are updating**
   - Print weight values before/after learning
   - Should change gradually

2. **Check predictions vary**
   - If always 0.5 ‚Üí not learning
   - Should adapt to patterns

3. **Check gradients**
   - Print gradient magnitudes
   - If ~0 ‚Üí vanishing gradients
   - If huge ‚Üí exploding gradients (clip!)

4. **Check inputs**
   - Are model predictions being fed correctly?
   - Are they normalized?

5. **Try simpler version**
   - Single neuron first
   - Then single gate
   - Then full LSTM

---

## üöÄ IMMEDIATE NEXT STEPS (TODAY)

### **This Afternoon:**

**1. Create LstmMixer.hpp skeleton (30 min)**
```cpp
// Basic class structure
// Constructor
// predict() method stub
// learn() method stub
```

**2. Implement sigmoid/tanh (if not in PAQ8px) (15 min)**
```cpp
float sigmoid(float x) { return 1.0 / (1.0 + exp(-x)); }
float tanh_activation(float x) { return tanh(x); }
```

**3. Implement single LSTM gate (1 hour)**
```cpp
// Just forget gate first
// Test it works
```

**4. Test standalone (30 min)**
```cpp
// Create simple test
// Feed dummy inputs
// Verify output
```

### **Tonight/Tomorrow:**

**5. Implement full LSTM (2-3 hours)**
- All three gates
- Forward pass complete
- Basic backward pass

**6. Integrate with PAQ8px (1-2 hours)**
- Add to mixer
- Collect predictions
- Feed to LSTM

**7. Compile and test (1 hour)**
- Fix compilation errors
- Run on tiny file first
- Verify doesn't crash

---

## üìà PROJECTED TIMELINE UPDATED

```
DAY 1 (TODAY - Dec 1):
‚îú‚îÄ Morning: ‚úÖ LSTM analysis DONE
‚îú‚îÄ Afternoon: Create skeleton + single gate
‚îî‚îÄ Evening: Test standalone LSTM

DAY 2 (Dec 2):
‚îú‚îÄ Morning: Complete LSTM implementation
‚îú‚îÄ Afternoon: Integrate with PAQ8px
‚îî‚îÄ Evening: Compile and debug

DAY 3 (Dec 3):
‚îú‚îÄ Morning: Test on small files
‚îú‚îÄ Afternoon: Tune hyperparameters
‚îî‚îÄ Evening: Launch 10 MB test

DAY 4 (Dec 4):
‚îú‚îÄ 10 MB test completes (2 hours)
‚îî‚îÄ Analyze results ‚Üí GO/NO-GO decision

IF GO:
DAY 5 (Dec 5):
‚îî‚îÄ Launch full enwik9 test (73 hours)

DAY 8 (Dec 8):
‚îî‚îÄ RESULTS! üéâ
```

---

## üéØ KEY DECISIONS MADE

### **1. Integration Approach: HYBRID ‚úÖ**
- Keep PAQ8px mixer
- Add LSTM as enhancement
- Safer, can fall back

### **2. Architecture: SIMPLE ‚úÖ**
- 1 layer, 48 cells
- Bit-level prediction
- Truncated BPTT

### **3. Training: ONLINE ‚úÖ**
- Learn during compression
- No separate training phase
- Adaptive to data

### **4. Testing: INCREMENTAL ‚úÖ**
- Standalone first
- Then integrate
- Then 10 MB
- Finally enwik9

---

## üí™ CONFIDENCE LEVEL

```
Technical Feasibility:  ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ (90%) - Code is clear
Implementation Time:    ‚úÖ‚úÖ‚úÖ‚úÖ‚ö™ (80%) - 3-5 days realistic
Expected Improvement:   ‚úÖ‚úÖ‚úÖ‚úÖ‚ö™ (75%) - STARLIT proved it works
World Record Chance:    ‚úÖ‚úÖ‚úÖ‚ö™‚ö™ (60%) - With non-linear scaling!
```

**Why high confidence:**
- ‚úÖ Proven technique (STARLIT won with it)
- ‚úÖ Clear implementation roadmap
- ‚úÖ Working code to learn from
- ‚úÖ Fast iteration possible (10 MB testing)
- ‚úÖ Multiple fallback options

---

## üéâ SUMMARY

### **What We Learned:**

1. **STARLIT uses classic LSTM with 3 gates** (forget, input, output)
2. **Online learning during compression** (perfect for our use case!)
3. **Adam optimizer for training** (adaptive learning rate)
4. **15-20 inputs from model predictions** (matches PAQ8px)
5. **Clear integration path** (hybrid mixer approach)

### **What We'll Build:**

1. **SimpleLstm class** (~200 lines of code)
2. **Integration with PAQ8px mixer** (~50 lines modification)
3. **Training loop** (~30 lines)
4. **Testing harness** (~50 lines)

**Total new code: ~330 lines**  
**That's VERY doable!** üöÄ

### **Expected Result:**

```
Conservative: 123.44 MB (4 MB improvement)
Realistic:    121.44 MB (6 MB improvement)
Optimistic:   119.44 MB (8 MB improvement)
Dream:        <114 MB (WORLD RECORD!)

Remember: 14x scaling factor before!
Could surprise us again! üéØ
```

---

## ‚úÖ TODAY'S ACHIEVEMENTS

- [x] **Deep dive STARLIT LSTM code**
- [x] **Understand architecture completely**
- [x] **Design adaptation plan**
- [x] **Create implementation roadmap**
- [x] **Identify key parameters**
- [x] **Set success metrics**
- [ ] **Create skeleton code** ‚Üê NEXT!

---

**Status:** ANALYSIS COMPLETE ‚úÖ  
**Next:** START IMPLEMENTATION!  
**Time:** Rest of Day 1  
**Goal:** Working LSTM skeleton by tonight  

**LET'S CODE! üöÄ**

---

*"We understand the technique. We have the plan. Now we build it!"*
